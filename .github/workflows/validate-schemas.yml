name: Validate Document Schemas

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'config/prompts/document_types/**'
      - 'config/schema.json'
      - 'scripts/batch_runner.py'
  pull_request:
    branches: [ main ]
    paths:
      - 'config/prompts/document_types/**'
      - 'config/schema.json'
      - 'scripts/batch_runner.py'
  workflow_dispatch:

jobs:
  validate-schemas:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    
    - name: Run schema validation
      run: |
        python scripts/batch_runner.py --verbose --report validation-report.json
    
    - name: Upload validation report
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: validation-report
        path: validation-report.json
        retention-days: 30
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request' && failure()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const report = JSON.parse(fs.readFileSync('validation-report.json', 'utf8'));
          
          let comment = '## ❌ Schema Validation Failed\n\n';
          comment += `- **Total**: ${report.summary.total}\n`;
          comment += `- **Passed**: ${report.summary.passed}\n`;
          comment += `- **Failed**: ${report.summary.failed}\n`;
          comment += `- **Warnings**: ${report.summary.warnings}\n\n`;
          
          if (report.results.failed.length > 0) {
            comment += '### Failed Validations:\n';
            report.results.failed.forEach(item => {
              comment += `- **${item.directory}**\n`;
            });
          }
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  validate-mcp-connectivity:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: agent_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: agent_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      neo4j:
        image: neo4j:5
        env:
          NEO4J_AUTH: neo4j/test_password
        ports:
          - 7687:7687
      
      qdrant:
        image: qdrant/qdrant
        ports:
          - 6333:6333
      
      redis:
        image: redis:7
        ports:
          - 6379:6379
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install test dependencies
      run: |
        python -m pip install --upgrade pip
        pip install python-dotenv psycopg2-binary neo4j qdrant-client redis
    
    - name: Create test environment file
      run: |
        cat > .env.mcp <<EOF
        POSTGRES_HOST=localhost
        POSTGRES_PORT=5432
        POSTGRES_DB=agent_db
        POSTGRES_USER=agent_user
        POSTGRES_PASSWORD=test_password
        NEO4J_URI=bolt://localhost:7687
        NEO4J_USER=neo4j
        NEO4J_PASSWORD=test_password
        QDRANT_URL=http://localhost:6333
        REDIS_URL=redis://localhost:6379/0
        EOF
    
    - name: Run MCP smoke test
      run: |
        python scripts/smoke_mcp.py
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: mcp-test-results
        path: |
          .env.mcp
          *.log
        retention-days: 7