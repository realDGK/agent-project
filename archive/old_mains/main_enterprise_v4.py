"""
Enterprise Real Estate Development Intelligence Platform v4.0
Multi-tenant SaaS-ready architecture for complex development portfolio management
"""
import os
import logging
from typing import Dict, Any, Optional, List, Tuple, Set
from datetime import datetime, timedelta
from decimal import Decimal
import json
import uuid
import re
from enum import Enum
import asyncio
import hashlib
from pathlib import Path

from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks, Depends, Query
from fastapi.responses import FileResponse, HTMLResponse, JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
import asyncpg
import google.generativeai as genai
from openai import OpenAI
import requests
import aiofiles
from sqlalchemy import create_engine, MetaData
from sqlalchemy.orm import Session

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configure AI models
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

app = FastAPI(
    title="Development Intelligence Platform", 
    version="4.0.0",
    description="Enterprise platform for real estate development portfolio management"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files for document browser
app.mount("/static", StaticFiles(directory="static", html=True), name="static")

DATABASE_URL = os.getenv(
    "DATABASE_URL", 
    "postgresql://agent_user:secure_agent_password_2025@localhost:5432/agent_orchestration"
)

# Cognee Knowledge Graph
COGNEE_ORCHESTRATOR_URL = "http://localhost:8001"
COGNEE_CONTRACTS_URL = "http://localhost:8002"

# Document storage path (for file browser)
DOCUMENT_STORAGE_PATH = os.getenv("DOCUMENT_STORAGE_PATH", "/home/scott/agent-project/document_repository")

# Ensure storage directories exist
Path(DOCUMENT_STORAGE_PATH).mkdir(parents=True, exist_ok=True)
Path(f"{DOCUMENT_STORAGE_PATH}/originals").mkdir(exist_ok=True)
Path(f"{DOCUMENT_STORAGE_PATH}/processed").mkdir(exist_ok=True)
Path(f"{DOCUMENT_STORAGE_PATH}/evidence").mkdir(exist_ok=True)

# Enhanced document classification for complex instruments
class DocumentType(Enum):
    # Core Transaction Documents
    LOI = "Letter of Intent"
    PSA = "Purchase and Sale Agreement"
    CLOSING = "Closing Statement"
    DEED = "Deed"
    
    # Development Documents
    DEVELOPMENT_AGREEMENT = "Development Agreement"
    SUBDIVISION_MAP = "Subdivision Map"
    VESTING_TENTATIVE_MAP = "Vesting Tentative Map"
    FINAL_MAP = "Final Map"
    PARCEL_MAP = "Parcel Map"
    
    # Complex Instruments
    REVERSION_AGREEMENT = "Reversion Agreement"
    PERFORMANCE_AGREEMENT = "Performance-Based Agreement"
    SETTLEMENT_AGREEMENT = "Settlement Agreement"
    JOINT_VENTURE = "Joint Venture Agreement"
    PROFIT_PARTICIPATION = "Profit Participation Agreement"
    
    # Infrastructure/Utilities
    SEWER_ALLOCATION = "Sewer Allocation"
    SEWER_CAPACITY = "Sewer Capacity Agreement"
    WATER_RIGHTS = "Water Rights"
    WATER_WILL_SERVE = "Water Will-Serve Letter"
    ROAD_AGREEMENT = "Road/Street Agreement"
    UTILITY_AGREEMENT = "Utility Agreement"
    IMPROVEMENT_AGREEMENT = "Public Improvement Agreement"
    
    # Bonds and Securities
    PERFORMANCE_BOND = "Performance Bond"
    PAYMENT_BOND = "Payment Bond"
    COMPLETION_BOND = "Completion Bond"
    MAINTENANCE_BOND = "Maintenance Bond"
    
    # Permits and Approvals
    BUILDING_PERMIT = "Building Permit"
    GRADING_PERMIT = "Grading Permit"
    CUP = "Conditional Use Permit"
    VARIANCE = "Variance"
    CEQA = "CEQA/Environmental"
    
    # Financial
    LOAN_AGREEMENT = "Loan Agreement"
    PROMISSORY_NOTE = "Promissory Note"
    DEED_OF_TRUST = "Deed of Trust"
    SUBORDINATION = "Subordination Agreement"
    
    # Correspondence
    EMAIL = "Email"
    MEMORANDUM = "Memorandum"
    NOTICE = "Notice"
    AMENDMENT = "Amendment"
    
    UNKNOWN = "Unknown"

class DealStatus(Enum):
    ACTIVE = "active"
    PENDING = "pending"
    CLOSED = "closed"
    DORMANT = "dormant"
    AT_RISK = "at_risk"
    PERFORMING = "performing"
    NON_PERFORMING = "non_performing"

class ObligationType(Enum):
    FINANCIAL = "financial"
    INFRASTRUCTURE = "infrastructure"
    REGULATORY = "regulatory"
    ENVIRONMENTAL = "environmental"
    PERFORMANCE = "performance"
    MAINTENANCE = "maintenance"
    REPORTING = "reporting"

class AssetType(Enum):
    LAND = "land"
    ENTITLEMENTS = "entitlements"
    SEWER_CAPACITY = "sewer_capacity"
    WATER_RIGHTS = "water_rights"
    DEVELOPMENT_RIGHTS = "development_rights"
    DENSITY_BONUS = "density_bonus"
    TAX_CREDITS = "tax_credits"
    IMPACT_FEE_CREDITS = "impact_fee_credits"

# Enhanced Models
class Asset(BaseModel):
    asset_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    asset_type: AssetType
    description: str
    location: Optional[str]
    value: Optional[Decimal]
    acquisition_date: Optional[datetime]
    status: str
    encumbrances: List[str] = []
    documents: List[str] = []
    performance_conditions: Optional[Dict] = None  # For reversion agreements
    metadata: Dict = {}

class Deal(BaseModel):
    deal_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str
    status: DealStatus
    properties: List[str] = []
    parties: List[Dict] = []
    total_value: Optional[Decimal]
    key_dates: Dict[str, datetime] = {}
    obligations: List[str] = []
    assets: List[Asset] = []
    risk_score: float = 0.0
    partner_notes: Optional[str]  # Preserved partner knowledge
    action_items: List[str] = []
    documents: List[str] = []
    
class PerformanceObligation(BaseModel):
    obligation_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    deal_id: Optional[str]
    property_id: Optional[str]
    type: ObligationType
    description: str
    trigger_conditions: Dict  # What triggers this obligation
    performance_metrics: Dict  # How to measure completion
    deadline: Optional[datetime]
    consequences: str  # What happens if not met
    status: str
    evidence_required: List[str] = []  # Documents needed to prove completion
    value_at_risk: Optional[Decimal]  # Financial impact if not met

class DocumentRepository(BaseModel):
    repository_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    tenant_id: str  # For multi-tenant SaaS
    document_id: str
    original_path: str
    processed_path: Optional[str]
    document_type: DocumentType
    sha256: str
    upload_date: datetime
    last_accessed: Optional[datetime]
    extracted_data: Dict = {}
    evidence_locations: List[Dict] = []  # Page/bbox for each claim
    validation_status: str
    confidence_score: float
    review_required: bool
    reviewed_by: Optional[str]
    review_date: Optional[datetime]
    tags: List[str] = []
    related_documents: List[str] = []

class MonetizationPath(BaseModel):
    path_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    deal_id: str
    strategy: str  # 'sell', 'develop', 'joint_venture', 'hold'
    requirements: List[str]  # What needs to be done
    timeline: Dict[str, datetime]
    estimated_value: Decimal
    risk_factors: List[str]
    partner_value_add: Optional[str]  # What partner brought to this path
    action_plan: List[Dict]

class FileNode(BaseModel):
    """For file browser interface"""
    name: str
    path: str
    type: str  # 'file' or 'directory'
    size: Optional[int]
    modified: datetime
    document_type: Optional[DocumentType]
    deal_id: Optional[str]
    tags: List[str] = []
    children: Optional[List['FileNode']] = None

# Database connection pool
db_pool = None

async def get_db_pool():
    global db_pool
    if not db_pool:
        try:
            db_pool = await asyncpg.create_pool(DATABASE_URL, min_size=2, max_size=10)
            logger.info("âœ… Database pool created")
        except Exception as e:
            logger.error(f"Database pool creation failed: {e}")
    return db_pool

# Enhanced startup with multi-tenant support
@app.on_event("startup")
async def startup():
    pool = await get_db_pool()
    if pool:
        # Create enhanced schema for enterprise features
        async with pool.acquire() as conn:
            await create_enterprise_schema(conn)
        
        logger.info("ðŸš€ Enterprise Development Intelligence Platform Started")
        logger.info("ðŸ“Š Features: Multi-tenant, Complex Instruments, Performance Tracking")
        
        # Initialize document repository structure
        await initialize_repository_structure()

async def create_enterprise_schema(conn):
    """Create enhanced database schema for enterprise features"""
    
    # Tenant table for SaaS
    await conn.execute("""
        CREATE TABLE IF NOT EXISTS tenants (
            tenant_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            name TEXT NOT NULL,
            subscription_tier TEXT DEFAULT 'standard',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            settings JSONB DEFAULT '{}'::jsonb
        );
    """)
    
    # Enhanced deals table
    await conn.execute("""
        CREATE TABLE IF NOT EXISTS deals (
            deal_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            tenant_id UUID REFERENCES tenants(tenant_id),
            name TEXT NOT NULL,
            status TEXT,
            total_value NUMERIC,
            risk_score NUMERIC,
            partner_notes TEXT,
            metadata JSONB DEFAULT '{}'::jsonb,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """)
    
    # Assets table for complex instruments
    await conn.execute("""
        CREATE TABLE IF NOT EXISTS assets (
            asset_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            tenant_id UUID REFERENCES tenants(tenant_id),
            deal_id UUID REFERENCES deals(deal_id),
            asset_type TEXT,
            description TEXT,
            location TEXT,
            value NUMERIC,
            acquisition_date DATE,
            status TEXT,
            performance_conditions JSONB,
            metadata JSONB DEFAULT '{}'::jsonb,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """)
    
    # Performance obligations with tracking
    await conn.execute("""
        CREATE TABLE IF NOT EXISTS performance_obligations (
            obligation_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            tenant_id UUID REFERENCES tenants(tenant_id),
            deal_id UUID REFERENCES deals(deal_id),
            type TEXT,
            description TEXT,
            trigger_conditions JSONB,
            performance_metrics JSONB,
            deadline DATE,
            consequences TEXT,
            status TEXT,
            value_at_risk NUMERIC,
            evidence_required JSONB,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            completed_at TIMESTAMP
        );
    """)
    
    # Document repository with evidence tracking
    await conn.execute("""
        CREATE TABLE IF NOT EXISTS document_repository (
            repository_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            tenant_id UUID REFERENCES tenants(tenant_id),
            document_id UUID,
            original_path TEXT,
            processed_path TEXT,
            document_type TEXT,
            sha256 TEXT UNIQUE,
            extracted_data JSONB,
            evidence_locations JSONB,
            validation_status TEXT,
            confidence_score NUMERIC,
            review_required BOOLEAN DEFAULT false,
            reviewed_by TEXT,
            review_date TIMESTAMP,
            tags TEXT[],
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            last_accessed TIMESTAMP
        );
    """)
    
    # Monetization paths
    await conn.execute("""
        CREATE TABLE IF NOT EXISTS monetization_paths (
            path_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            tenant_id UUID REFERENCES tenants(tenant_id),
            deal_id UUID REFERENCES deals(deal_id),
            strategy TEXT,
            requirements JSONB,
            timeline JSONB,
            estimated_value NUMERIC,
            risk_factors JSONB,
            partner_value_add TEXT,
            action_plan JSONB,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """)
    
    # Evidence tracking for claims
    await conn.execute("""
        CREATE TABLE IF NOT EXISTS evidence_tracking (
            evidence_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            tenant_id UUID REFERENCES tenants(tenant_id),
            claim TEXT NOT NULL,
            source_document UUID REFERENCES document_repository(repository_id),
            page INTEGER,
            bbox TEXT,  -- x1,y1,x2,y2
            confidence NUMERIC,
            validated BOOLEAN DEFAULT false,
            validator TEXT,
            validation_date TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """)
    
    # Audit trail for HIL reviews
    await conn.execute("""
        CREATE TABLE IF NOT EXISTS audit_trail (
            audit_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            tenant_id UUID REFERENCES tenants(tenant_id),
            user_id TEXT,
            action TEXT,
            entity_type TEXT,
            entity_id UUID,
            old_value JSONB,
            new_value JSONB,
            reason TEXT,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """)
    
    # Create indexes for performance
    await conn.execute("""
        CREATE INDEX IF NOT EXISTS idx_deals_status ON deals(status);
        CREATE INDEX IF NOT EXISTS idx_obligations_deadline ON performance_obligations(deadline);
        CREATE INDEX IF NOT EXISTS idx_obligations_status ON performance_obligations(status);
        CREATE INDEX IF NOT EXISTS idx_assets_type ON assets(asset_type);
        CREATE INDEX IF NOT EXISTS idx_repository_type ON document_repository(document_type);
        CREATE INDEX IF NOT EXISTS idx_repository_sha256 ON document_repository(sha256);
        CREATE INDEX IF NOT EXISTS idx_evidence_document ON evidence_tracking(source_document);
    """)
    
    logger.info("âœ… Enterprise schema created")

async def initialize_repository_structure():
    """Initialize document repository folder structure"""
    base_dirs = [
        "originals",
        "processed", 
        "evidence",
        "temp",
        "archive"
    ]
    
    for dir_name in base_dirs:
        dir_path = Path(DOCUMENT_STORAGE_PATH) / dir_name
        dir_path.mkdir(parents=True, exist_ok=True)
    
    # Create category subdirectories
    categories = [
        "contracts", "permits", "correspondence", 
        "financials", "maps", "reports", "legal"
    ]
    
    for category in categories:
        (Path(DOCUMENT_STORAGE_PATH) / "originals" / category).mkdir(exist_ok=True)
        (Path(DOCUMENT_STORAGE_PATH) / "processed" / category).mkdir(exist_ok=True)

# Main dashboard with enterprise features
@app.get("/")
async def root():
    return FileResponse("static/index.html")

@app.get("/api/dashboard/enterprise")
async def enterprise_dashboard(tenant_id: str = Query(default="default")):
    """Enterprise dashboard with complete portfolio view"""
    
    pool = await get_db_pool()
    if not pool:
        raise HTTPException(status_code=503, detail="Database unavailable")
    
    async with pool.acquire() as conn:
        # Get active deals
        deals = await conn.fetch("""
            SELECT d.*, 
                   COUNT(DISTINCT po.obligation_id) as obligation_count,
                   COUNT(DISTINCT a.asset_id) as asset_count,
                   SUM(po.value_at_risk) as total_risk
            FROM deals d
            LEFT JOIN performance_obligations po ON d.deal_id = po.deal_id
            LEFT JOIN assets a ON d.deal_id = a.deal_id
            WHERE d.status IN ('active', 'at_risk', 'performing')
            GROUP BY d.deal_id
            ORDER BY d.risk_score DESC
        """)
        
        # Get critical obligations (due in 30 days or overdue)
        critical_obligations = await conn.fetch("""
            SELECT po.*, d.name as deal_name
            FROM performance_obligations po
            JOIN deals d ON po.deal_id = d.deal_id
            WHERE po.status != 'completed'
            AND (po.deadline <= CURRENT_DATE + INTERVAL '30 days' 
                 OR po.deadline < CURRENT_DATE)
            ORDER BY po.deadline
        """)
        
        # Get valuable assets
        assets = await conn.fetch("""
            SELECT a.*, d.name as deal_name
            FROM assets a
            LEFT JOIN deals d ON a.deal_id = d.deal_id
            WHERE a.value > 0
            ORDER BY a.value DESC
            LIMIT 20
        """)
        
        # Get monetization opportunities
        opportunities = await conn.fetch("""
            SELECT mp.*, d.name as deal_name
            FROM monetization_paths mp
            JOIN deals d ON mp.deal_id = d.deal_id
            ORDER BY mp.estimated_value DESC
            LIMIT 10
        """)
        
        # Calculate portfolio summary
        portfolio_summary = await conn.fetchrow("""
            SELECT 
                COUNT(DISTINCT d.deal_id) as total_deals,
                SUM(d.total_value) as portfolio_value,
                SUM(po.value_at_risk) as total_risk,
                COUNT(DISTINCT a.asset_id) as total_assets,
                SUM(a.value) as asset_value
            FROM deals d
            LEFT JOIN performance_obligations po ON d.deal_id = po.deal_id
            LEFT JOIN assets a ON d.deal_id = a.deal_id
        """)
        
        # Get documents requiring review
        pending_reviews = await conn.fetch("""
            SELECT dr.*, 
                   COUNT(et.evidence_id) as evidence_count
            FROM document_repository dr
            LEFT JOIN evidence_tracking et ON dr.repository_id = et.source_document
            WHERE dr.review_required = true
            AND dr.reviewed_by IS NULL
            GROUP BY dr.repository_id
            ORDER BY dr.created_at DESC
            LIMIT 10
        """)
    
    return {
        "portfolio_summary": {
            "total_deals": portfolio_summary['total_deals'] or 0,
            "portfolio_value": float(portfolio_summary['portfolio_value'] or 0),
            "total_risk": float(portfolio_summary['total_risk'] or 0),
            "total_assets": portfolio_summary['total_assets'] or 0,
            "asset_value": float(portfolio_summary['asset_value'] or 0)
        },
        "active_deals": [dict(d) for d in deals],
        "critical_obligations": [dict(o) for o in critical_obligations],
        "valuable_assets": [dict(a) for a in assets],
        "monetization_opportunities": [dict(o) for o in opportunities],
        "pending_reviews": [dict(r) for r in pending_reviews],
        "alerts": await generate_alerts(critical_obligations, deals)
    }

async def generate_alerts(obligations, deals):
    """Generate intelligent alerts based on portfolio status"""
    alerts = []
    
    # Check for overdue obligations
    overdue = [o for o in obligations if o['deadline'] and o['deadline'] < datetime.now().date()]
    if overdue:
        alerts.append({
            "type": "critical",
            "message": f"{len(overdue)} obligations are overdue",
            "action": "Review immediately to avoid consequences"
        })
    
    # Check for high-risk deals
    high_risk = [d for d in deals if d['risk_score'] and d['risk_score'] > 0.7]
    if high_risk:
        alerts.append({
            "type": "warning",
            "message": f"{len(high_risk)} deals at high risk",
            "action": "Review risk mitigation strategies"
        })
    
    # Check for expiring permits/entitlements
    # Would query permit expiration dates
    
    return alerts

@app.post("/api/documents/process-complex")
async def process_complex_document(
    file: UploadFile = File(...),
    tenant_id: str = Query(default="default"),
    force_deep_analysis: bool = Query(default=False)
):
    """Process complex documents with performance tracking"""
    
    try:
        # Save original document
        file_content = await file.read()
        file_hash = hashlib.sha256(file_content).hexdigest()
        
        original_path = Path(DOCUMENT_STORAGE_PATH) / "originals" / f"{file_hash}_{file.filename}"
        async with aiofiles.open(original_path, 'wb') as f:
            await f.write(file_content)
        
        # Extract text
        text_content = await extract_text_advanced(file_content, file.filename)
        
        # Classify document
        doc_type, subtype = await classify_complex_document(text_content, file.filename)
        
        # Determine processing depth
        if force_deep_analysis or doc_type in [
            DocumentType.DEVELOPMENT_AGREEMENT,
            DocumentType.REVERSION_AGREEMENT,
            DocumentType.SETTLEMENT_AGREEMENT,
            DocumentType.SEWER_ALLOCATION
        ]:
            extraction_result = await deep_complex_extraction(text_content, file.filename, doc_type)
        else:
            extraction_result = await standard_extraction(text_content, file.filename, doc_type)
        
        # Extract performance obligations
        obligations = await extract_performance_obligations(text_content, doc_type)
        
        # Extract assets (sewer allocations, water rights, etc.)
        assets = await extract_assets(text_content, doc_type)
        
        # Identify deal relationships
        deal_context = await identify_deal_context(text_content, extraction_result)
        
        # Store in repository
        pool = await get_db_pool()
        if pool:
            async with pool.acquire() as conn:
                repo_id = await conn.fetchval("""
                    INSERT INTO document_repository 
                    (tenant_id, original_path, document_type, sha256, 
                     extracted_data, confidence_score, review_required)
                    VALUES ($1, $2, $3, $4, $5, $6, $7)
                    RETURNING repository_id
                """, 
                    uuid.UUID(tenant_id) if tenant_id != "default" else None,
                    str(original_path),
                    doc_type.value,
                    file_hash,
                    json.dumps(extraction_result),
                    extraction_result.get('confidence', 0.8),
                    extraction_result.get('confidence', 0.8) < 0.85
                )
                
                # Store obligations
                for obligation in obligations:
                    await conn.execute("""
                        INSERT INTO performance_obligations
                        (tenant_id, type, description, trigger_conditions, 
                         performance_metrics, deadline, consequences, status, value_at_risk)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                    """,
                        uuid.UUID(tenant_id) if tenant_id != "default" else None,
                        obligation['type'],
                        obligation['description'],
                        json.dumps(obligation.get('triggers', {})),
                        json.dumps(obligation.get('metrics', {})),
                        obligation.get('deadline'),
                        obligation.get('consequences', ''),
                        'pending',
                        obligation.get('value_at_risk', 0)
                    )
                
                # Store assets
                for asset in assets:
                    await conn.execute("""
                        INSERT INTO assets
                        (tenant_id, asset_type, description, location, value, status, metadata)
                        VALUES ($1, $2, $3, $4, $5, $6, $7)
                    """,
                        uuid.UUID(tenant_id) if tenant_id != "default" else None,
                        asset['type'],
                        asset['description'],
                        asset.get('location'),
                        asset.get('value', 0),
                        asset.get('status', 'active'),
                        json.dumps(asset)
                    )
                
                # Update Cognee knowledge graph
                await update_knowledge_graph(repo_id, extraction_result, obligations, assets)
        
        return {
            "repository_id": str(repo_id),
            "document_type": doc_type.value,
            "confidence": extraction_result.get('confidence', 0.8),
            "obligations_found": len(obligations),
            "assets_found": len(assets),
            "deal_context": deal_context,
            "review_required": extraction_result.get('confidence', 0.8) < 0.85,
            "extraction_summary": extraction_result.get('summary', {}),
            "evidence_tracking": extraction_result.get('evidence', [])
        }
        
    except Exception as e:
        logger.exception("Complex document processing failed")
        raise HTTPException(status_code=500, detail=str(e))

async def extract_performance_obligations(text: str, doc_type: DocumentType) -> List[Dict]:
    """Extract performance-based obligations from documents"""
    
    obligations = []
    
    # Pattern matching for performance obligations
    performance_patterns = [
        r'(?:shall|must|required to)\s+(?:complete|perform|deliver|construct)',
        r'(?:upon|after)\s+(?:completion|performance|satisfaction)',
        r'(?:condition|contingent|subject to)',
        r'(?:milestone|benchmark|target)',
        r'(?:reversion|revert|forfeit)',
        r'(?:failure to)\s+(?:complete|perform|meet)',
    ]
    
    # Use AI for complex extraction
    if GOOGLE_API_KEY:
        try:
            model = genai.GenerativeModel('gemini-1.5-pro')
            prompt = f"""
            Extract ALL performance obligations from this {doc_type.value}:
            
            For each obligation, identify:
            1. Description of what must be performed
            2. Trigger conditions (what starts the obligation)
            3. Performance metrics (how completion is measured)
            4. Deadline or timeline
            5. Consequences of non-performance
            6. Value at risk (financial impact)
            
            Focus especially on:
            - Reversion clauses (land reverting if conditions not met)
            - Infrastructure obligations (roads, sewer, utilities)
            - Development milestones
            - Financial performance requirements
            
            Return JSON array of obligations.
            
            Document text:
            {text[:10000]}
            """
            
            response = model.generate_content(prompt)
            result = json.loads(response.text.strip().replace('```json', '').replace('```', ''))
            obligations.extend(result if isinstance(result, list) else [])
            
        except Exception as e:
            logger.error(f"AI extraction failed: {e}")
    
    # Pattern-based extraction as fallback
    for pattern in performance_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            context = text[max(0, match.start()-200):min(len(text), match.end()+200)]
            obligations.append({
                "type": "performance",
                "description": context,
                "pattern_matched": pattern,
                "requires_review": True
            })
    
    return obligations

async def extract_assets(text: str, doc_type: DocumentType) -> List[Dict]:
    """Extract valuable assets from documents"""
    
    assets = []
    
    # Specific asset patterns
    asset_patterns = {
        AssetType.SEWER_CAPACITY: [
            r'(\d+)\s*(?:EDU|equivalent dwelling unit)',
            r'sewer\s+(?:capacity|allocation)\s+(?:of|for)\s+(\d+)',
            r'wastewater\s+treatment\s+capacity',
        ],
        AssetType.WATER_RIGHTS: [
            r'water\s+(?:rights|allocation)',
            r'(\d+)\s*(?:acre-feet|af)\s+(?:of\s+)?water',
            r'well\s+(?:permit|rights)',
        ],
        AssetType.DEVELOPMENT_RIGHTS: [
            r'(?:density|development)\s+(?:bonus|rights)',
            r'(?:transferable|tradeable)\s+development',
            r'(\d+)\s+(?:additional\s+)?units',
        ],
        AssetType.IMPACT_FEE_CREDITS: [
            r'(?:impact|development)\s+fee\s+(?:credit|offset)',
            r'(?:park|school|traffic)\s+fee\s+credit',
            r'\$[\d,]+\s+(?:in\s+)?(?:fee\s+)?credits?',
        ]
    }
    
    for asset_type, patterns in asset_patterns.items():
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                context = text[max(0, match.start()-200):min(len(text), match.end()+200)]
                
                # Try to extract value
                value_match = re.search(r'\$[\d,]+(?:\.\d{2})?', context)
                value = None
                if value_match:
                    value = float(value_match.group().replace('$', '').replace(',', ''))
                
                assets.append({
                    "type": asset_type.value,
                    "description": context.strip(),
                    "value": value,
                    "pattern_matched": pattern,
                    "document_source": doc_type.value
                })
    
    return assets

async def identify_deal_context(text: str, extraction_result: Dict) -> Dict:
    """Identify which deal a document belongs to"""
    
    context = {
        "deal_name": None,
        "properties": [],
        "parties": extraction_result.get('parties', []),
        "related_deals": [],
        "status_indicators": []
    }
    
    # Look for project/deal names
    project_patterns = [
        r'(?:project|development)\s*:\s*([^\\n]+)',
        r're:\s*([^\\n]+)',
        r'(?:property|site)\s*(?:known as|called)\s*([^\\n]+)',
    ]
    
    for pattern in project_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            context['deal_name'] = match.group(1).strip()
            break
    
    # Look for status indicators
    status_patterns = {
        'active': ['currently', 'ongoing', 'in progress'],
        'pending': ['proposed', 'tentative', 'subject to'],
        'closed': ['completed', 'closed', 'final'],
        'at_risk': ['default', 'breach', 'termination', 'reversion']
    }
    
    for status, keywords in status_patterns.items():
        if any(keyword in text.lower() for keyword in keywords):
            context['status_indicators'].append(status)
    
    return context

@app.get("/api/documents/browser")
async def document_browser(
    path: str = Query(default="/"),
    tenant_id: str = Query(default="default")
) -> List[FileNode]:
    """File browser API for document repository"""
    
    base_path = Path(DOCUMENT_STORAGE_PATH)
    current_path = base_path / path.lstrip('/')
    
    if not current_path.exists() or not current_path.is_dir():
        raise HTTPException(status_code=404, detail="Path not found")
    
    nodes = []
    
    try:
        for item in current_path.iterdir():
            node = FileNode(
                name=item.name,
                path=str(item.relative_to(base_path)),
                type="directory" if item.is_dir() else "file",
                size=item.stat().st_size if item.is_file() else None,
                modified=datetime.fromtimestamp(item.stat().st_mtime)
            )
            
            # Add metadata if file is in database
            if item.is_file():
                pool = await get_db_pool()
                if pool:
                    async with pool.acquire() as conn:
                        doc_info = await conn.fetchrow("""
                            SELECT document_type, tags, confidence_score
                            FROM document_repository
                            WHERE original_path = $1
                        """, str(item))
                        
                        if doc_info:
                            node.document_type = DocumentType(doc_info['document_type'])
                            node.tags = doc_info['tags'] or []
            
            nodes.append(node)
    
    except Exception as e:
        logger.error(f"Error browsing documents: {e}")
        raise HTTPException(status_code=500, detail="Error browsing documents")
    
    # Sort directories first, then files
    nodes.sort(key=lambda x: (x.type != "directory", x.name.lower()))
    
    return nodes

@app.post("/api/evidence/validate")
async def validate_evidence(
    claim: str,
    document_id: str,
    page: int,
    bbox: str,  # x1,y1,x2,y2
    tenant_id: str = Query(default="default")
):
    """Validate a claim with evidence tracking"""
    
    pool = await get_db_pool()
    if not pool:
        raise HTTPException(status_code=503, detail="Database unavailable")
    
    async with pool.acquire() as conn:
        # Check if document exists
        doc = await conn.fetchrow("""
            SELECT repository_id, processed_path, extracted_data
            FROM document_repository
            WHERE repository_id = $1
        """, uuid.UUID(document_id))
        
        if not doc:
            raise HTTPException(status_code=404, detail="Document not found")
        
        # Extract text from bbox location
        # This would use PDF processing to get exact text
        extracted_text = await extract_text_from_bbox(doc['processed_path'], page, bbox)
        
        # Validate claim against extracted text
        validation_result = await validate_claim_against_text(claim, extracted_text)
        
        # Store evidence
        evidence_id = await conn.fetchval("""
            INSERT INTO evidence_tracking
            (tenant_id, claim, source_document, page, bbox, confidence, validated, validator)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            RETURNING evidence_id
        """,
            uuid.UUID(tenant_id) if tenant_id != "default" else None,
            claim,
            doc['repository_id'],
            page,
            bbox,
            validation_result['confidence'],
            validation_result['is_valid'],
            'system'
        )
        
        # Add audit trail
        await conn.execute("""
            INSERT INTO audit_trail
            (tenant_id, user_id, action, entity_type, entity_id, new_value)
            VALUES ($1, $2, $3, $4, $5, $6)
        """,
            uuid.UUID(tenant_id) if tenant_id != "default" else None,
            'system',
            'evidence_validation',
            'claim',
            evidence_id,
            json.dumps(validation_result)
        )
    
    return {
        "evidence_id": str(evidence_id),
        "claim": claim,
        "is_valid": validation_result['is_valid'],
        "confidence": validation_result['confidence'],
        "extracted_text": extracted_text,
        "explanation": validation_result.get('explanation')
    }

@app.get("/api/monetization/paths/{deal_id}")
async def get_monetization_paths(
    deal_id: str,
    tenant_id: str = Query(default="default")
) -> List[MonetizationPath]:
    """Get monetization strategies for a deal"""
    
    pool = await get_db_pool()
    if not pool:
        raise HTTPException(status_code=503, detail="Database unavailable")
    
    async with pool.acquire() as conn:
        # Get deal details
        deal = await conn.fetchrow("""
            SELECT * FROM deals WHERE deal_id = $1
        """, uuid.UUID(deal_id))
        
        if not deal:
            raise HTTPException(status_code=404, detail="Deal not found")
        
        # Get existing paths
        existing_paths = await conn.fetch("""
            SELECT * FROM monetization_paths
            WHERE deal_id = $1
            ORDER BY estimated_value DESC
        """, uuid.UUID(deal_id))
        
        # Generate new paths based on assets and obligations
        new_paths = await generate_monetization_strategies(deal_id, conn)
        
        # Combine and return
        all_paths = []
        for path in existing_paths:
            all_paths.append(MonetizationPath(**dict(path)))
        
        for path in new_paths:
            all_paths.append(path)
        
        return all_paths

async def generate_monetization_strategies(deal_id: str, conn) -> List[MonetizationPath]:
    """Generate intelligent monetization strategies"""
    
    strategies = []
    
    # Get deal assets and obligations
    assets = await conn.fetch("""
        SELECT * FROM assets WHERE deal_id = $1
    """, uuid.UUID(deal_id))
    
    obligations = await conn.fetch("""
        SELECT * FROM performance_obligations 
        WHERE deal_id = $1 AND status != 'completed'
    """, uuid.UUID(deal_id))
    
    # Strategy 1: Quick Sale (if minimal obligations)
    if len(obligations) < 3:
        strategies.append(MonetizationPath(
            deal_id=deal_id,
            strategy="quick_sale",
            requirements=["Clear title", "Resolve any liens", "Market analysis"],
            timeline={"listing": datetime.now() + timedelta(days=30)},
            estimated_value=Decimal(sum(a['value'] or 0 for a in assets) * 0.9),
            risk_factors=["Market conditions", "Buyer financing"],
            action_plan=[
                {"step": 1, "action": "Order title report"},
                {"step": 2, "action": "Get broker opinion of value"},
                {"step": 3, "action": "List property"}
            ]
        ))
    
    # Strategy 2: Complete Development (if entitlements exist)
    if any(a['asset_type'] == 'entitlements' for a in assets):
        strategies.append(MonetizationPath(
            deal_id=deal_id,
            strategy="complete_development",
            requirements=["Complete infrastructure", "Obtain permits", "Secure financing"],
            timeline={"completion": datetime.now() + timedelta(days=365)},
            estimated_value=Decimal(sum(a['value'] or 0 for a in assets) * 2.5),
            risk_factors=["Construction delays", "Cost overruns", "Market changes"],
            partner_value_add="Partner had relationships with contractors and city staff",
            action_plan=[
                {"step": 1, "action": "Review development agreement"},
                {"step": 2, "action": "Update construction budget"},
                {"step": 3, "action": "Secure construction financing"}
            ]
        ))
    
    # Strategy 3: Joint Venture (if complex obligations)
    if len(obligations) > 5:
        strategies.append(MonetizationPath(
            deal_id=deal_id,
            strategy="joint_venture",
            requirements=["Find partner", "Negotiate terms", "Legal documentation"],
            timeline={"partner_selection": datetime.now() + timedelta(days=60)},
            estimated_value=Decimal(sum(a['value'] or 0 for a in assets) * 1.5),
            risk_factors=["Partner alignment", "Profit sharing", "Control issues"],
            partner_value_add="Partner's expertise in navigating complex agreements",
            action_plan=[
                {"step": 1, "action": "Prepare investment package"},
                {"step": 2, "action": "Identify potential partners"},
                {"step": 3, "action": "Negotiate JV agreement"}
            ]
        ))
    
    return strategies

@app.post("/api/hil/review")
async def submit_hil_review(
    document_id: str,
    review_notes: str,
    corrections: Dict,
    approved: bool,
    reviewer: str,
    tenant_id: str = Query(default="default")
):
    """Submit human review of document extraction"""
    
    pool = await get_db_pool()
    if not pool:
        raise HTTPException(status_code=503, detail="Database unavailable")
    
    async with pool.acquire() as conn:
        # Update document
        await conn.execute("""
            UPDATE document_repository
            SET review_required = false,
                reviewed_by = $1,
                review_date = CURRENT_TIMESTAMP,
                validation_status = $2,
                extracted_data = extracted_data || $3
            WHERE repository_id = $4
        """,
            reviewer,
            'approved' if approved else 'needs_correction',
            json.dumps(corrections),
            uuid.UUID(document_id)
        )
        
        # Add audit trail
        await conn.execute("""
            INSERT INTO audit_trail
            (tenant_id, user_id, action, entity_type, entity_id, new_value, reason)
            VALUES ($1, $2, $3, $4, $5, $6, $7)
        """,
            uuid.UUID(tenant_id) if tenant_id != "default" else None,
            reviewer,
            'document_review',
            'document',
            uuid.UUID(document_id),
            json.dumps({"approved": approved, "corrections": corrections}),
            review_notes
        )
    
    return {"status": "review_submitted", "document_id": document_id}

# Helper functions
async def extract_text_advanced(file_content: bytes, filename: str) -> str:
    """Advanced text extraction with OCR fallback"""
    # Implementation would use pymupdf, pdfplumber, or tesseract
    return file_content.decode('utf-8', errors='ignore')

async def classify_complex_document(text: str, filename: str) -> Tuple[DocumentType, str]:
    """Classify complex real estate documents"""
    # Your existing classification logic enhanced
    return DocumentType.UNKNOWN, "unknown"

async def deep_complex_extraction(text: str, filename: str, doc_type: DocumentType) -> Dict:
    """Deep extraction for complex instruments"""
    # Use Gemini Pro or GPT-4 for complex extraction
    return {}

async def standard_extraction(text: str, filename: str, doc_type: DocumentType) -> Dict:
    """Standard extraction using Gemini Flash"""
    # Your existing extraction logic
    return {}

async def extract_text_from_bbox(doc_path: str, page: int, bbox: str) -> str:
    """Extract text from specific bbox location"""
    # Would use pymupdf or similar
    return "Extracted text from bbox"

async def validate_claim_against_text(claim: str, text: str) -> Dict:
    """Validate if claim is supported by text"""
    # Use AI to validate
    return {"is_valid": True, "confidence": 0.95}

async def update_knowledge_graph(repo_id: str, extraction: Dict, obligations: List, assets: List):
    """Update Cognee knowledge graph"""
    try:
        # Add to Cognee
        response = requests.post(
            f"{COGNEE_ORCHESTRATOR_URL}/api/add",
            json={
                "type": "document",
                "id": str(repo_id),
                "extraction": extraction,
                "obligations": obligations,
                "assets": assets
            },
            timeout=5
        )
        logger.info(f"Updated Cognee graph for document {repo_id}")
    except Exception as e:
        logger.error(f"Cognee update failed: {e}")

@app.get("/health")
async def health():
    """Health check"""
    return {
        "status": "healthy",
        "version": "4.0.0",
        "features": [
            "enterprise_multi_tenant",
            "complex_instruments",
            "performance_tracking",
            "monetization_paths",
            "evidence_validation",
            "document_browser"
        ]
    }

if __name__ == "__main__":
    import uvicorn
    print("\n" + "="*60)
    print("ENTERPRISE DEVELOPMENT INTELLIGENCE PLATFORM v4.0")
    print("="*60)
    print("\nðŸ“‹ Enterprise Features:")
    print("  â€¢ Multi-tenant SaaS architecture")
    print("  â€¢ Complex instrument tracking (reversions, allocations)")
    print("  â€¢ Performance obligation monitoring")
    print("  â€¢ Asset valuation and tracking")
    print("  â€¢ Monetization path generation")
    print("  â€¢ Evidence-based claim validation")
    print("  â€¢ Document repository browser")
    print("\nðŸš€ Starting server on http://localhost:8000")
    print("="*60 + "\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)
